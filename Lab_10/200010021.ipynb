{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZDMbDP0uJN0"
      },
      "source": [
        "# **LAB 10 : Hidden Markov Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MKFudLjzt_K5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64j_rFZeF57i"
      },
      "source": [
        "Please refer to the following [article](http://www.adeveloperdiary.com/data-science/machine-learning/introduction-to-hidden-markov-model/) to understand Hidden Markov Model\n",
        "\n",
        "Here we will be dealing with 3 major problems : \n",
        "  \n",
        "  1. Evaluation Problem\n",
        "  2. Learning Problem\n",
        "  3. Decoding Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEasU5TNGNbE"
      },
      "source": [
        "1. Evaluation Problem : Implementation of Forward and Backward Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjUe9joAGXvC",
        "outputId": "0a1ef95b-9653-4518-adbc-d288b6343da5"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('data_python.csv') ## Read the data, change the path accordingly\n",
        "\n",
        "V = data['Visible'].values\n",
        "\n",
        "# Transition Probabilities\n",
        "a = np.array(((0.54, 0.46), (0.49, 0.51)))\n",
        "\n",
        "# Emission Probabilities\n",
        "b = np.array(((0.16, 0.26, 0.58), (0.25, 0.28, 0.47)))\n",
        "\n",
        "# Equal Probabilities for the initial distribution\n",
        "initial_distribution = np.array((0.5, 0.5))\n",
        "\n",
        "\n",
        "def forward(V, a, b, initial_distribution):\n",
        "    alpha = np.zeros((V.shape[0], a.shape[0]))\n",
        "    \n",
        "    ## Write your code here\n",
        "    alpha[0, :] = initial_distribution * b[:, V[0]]\n",
        " \n",
        "    for t in range(1, V.shape[0]):\n",
        "        for j in range(a.shape[0]):\n",
        "            alpha[t, j] = alpha[t - 1].dot(a[:, j]) * b[j, V[t]]\n",
        " \n",
        "    return alpha\n",
        "\n",
        "\n",
        "alpha = forward(V, a, b, initial_distribution)\n",
        "\n",
        "\n",
        "def backward(V, a, b):\n",
        "    beta = np.zeros((V.shape[0], a.shape[0]))\n",
        "\n",
        "    ## Write your code here\n",
        "    beta[V.shape[0] - 1] = np.ones((a.shape[0]))\n",
        " \n",
        "    # Loop in backward way from T-1 to\n",
        "    # Due to python indexing the actual loop will be T-2 to 0\n",
        "    for t in range(V.shape[0] - 2, -1, -1):\n",
        "        for j in range(a.shape[0]):\n",
        "            beta[t, j] = (beta[t + 1] * b[:, V[t + 1]]).dot(a[j, :])\n",
        " \n",
        "    return beta\n",
        "\n",
        "\n",
        "beta = backward(V, a, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Iyc4mIlGzPk"
      },
      "source": [
        "2. Learning Problem : Implementation of Baum Welch Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6eaCtNEG3FQ",
        "outputId": "143855ca-81f4-4bfb-97f9-5a40596c15eb"
      },
      "outputs": [],
      "source": [
        "def baum_welch(V, a, b, initial_distribution, n_iter=100):\n",
        "    M = a.shape[0]\n",
        "    T = len(V)\n",
        " \n",
        "    for n in range(n_iter):\n",
        "        alpha = forward(V, a, b, initial_distribution)\n",
        "        beta = backward(V, a, b)\n",
        " \n",
        "        xi = np.zeros((M, M, T - 1))\n",
        "        for t in range(T - 1):\n",
        "            denominator = np.dot(np.dot(alpha[t, :].T, a) * b[:, V[t + 1]].T, beta[t + 1, :])\n",
        "            for i in range(M):\n",
        "                numerator = alpha[t, i] * a[i, :] * b[:, V[t + 1]].T * beta[t + 1, :].T\n",
        "                xi[i, :, t] = numerator / denominator\n",
        " \n",
        "        gamma = np.sum(xi, axis=1)\n",
        "        a = np.sum(xi, 2) / np.sum(gamma, axis=1).reshape((-1, 1))\n",
        " \n",
        "        # Add additional T'th element in gamma\n",
        "        gamma = np.hstack((gamma, np.sum(xi[:, :, T - 2], axis=0).reshape((-1, 1))))\n",
        " \n",
        "        K = b.shape[1]\n",
        "        denominator = np.sum(gamma, axis=1)\n",
        "        for l in range(K):\n",
        "            b[:, l] = np.sum(gamma[:, V == l], axis=1)\n",
        " \n",
        "        b = np.divide(b, denominator.reshape((-1, 1)))\n",
        "\n",
        "    return (a,b)\n",
        "\n",
        "data = pd.read_csv('data_python.csv')\n",
        "\n",
        "V = data['Visible'].values\n",
        "\n",
        "# Transition Probabilities\n",
        "a = np.ones((2, 2))\n",
        "a = a / np.sum(a, axis=1)\n",
        "\n",
        "# Emission Probabilities\n",
        "b = np.array(((1, 3, 5), (2, 4, 6)))\n",
        "b = b / np.sum(b, axis=1).reshape((-1, 1))\n",
        "\n",
        "# Equal Probabilities for the initial distribution\n",
        "initial_distribution = np.array((0.5, 0.5))\n",
        "\n",
        "a,b = baum_welch(V, a, b, initial_distribution, n_iter=100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A40-TPXZHTHE"
      },
      "source": [
        "3. Decoding Problem : Implementation of Viterbi Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBG74dknHYgM",
        "outputId": "1ac04c53-fea0-4047-aae0-768c6c6e0780"
      },
      "outputs": [],
      "source": [
        "def viterbi(V, a, b, initial_distribution):\n",
        "    T = V.shape[0]\n",
        "    M = a.shape[0]\n",
        " \n",
        "    omega = np.zeros((T, M))\n",
        "    omega[0, :] = np.log(initial_distribution * b[:, V[0]])\n",
        " \n",
        "    prev = np.zeros((T - 1, M))\n",
        " \n",
        "    for t in range(1, T):\n",
        "        for j in range(M):\n",
        "            # Same as Forward Probability\n",
        "            probability = omega[t - 1] + np.log(a[:, j]) + np.log(b[j, V[t]])\n",
        " \n",
        "            # This is our most probable state given previous state at time t (1)\n",
        "            prev[t - 1, j] = np.argmax(probability)\n",
        " \n",
        "            # This is the probability of the most probable state (2)\n",
        "            omega[t, j] = np.max(probability)\n",
        " \n",
        "    # Path Array\n",
        "    S = np.zeros(T)\n",
        " \n",
        "    # Find the most probable last hidden state\n",
        "    last_state = np.argmax(omega[T - 1, :])\n",
        " \n",
        "    S[0] = last_state\n",
        " \n",
        "    backtrack_index = 1\n",
        "    for i in range(T - 2, -1, -1):\n",
        "        S[backtrack_index] = prev[i, int(last_state)]\n",
        "        last_state = prev[i, int(last_state)]\n",
        "        backtrack_index += 1\n",
        " \n",
        "    # Flip the path array since we were backtracking\n",
        "    S = np.flip(S, axis=0)\n",
        " \n",
        "    # Convert numeric values to actual hidden states\n",
        "    result = []\n",
        "    for s in S:\n",
        "        if s == 0:\n",
        "            result.append(\"A\")\n",
        "        else:\n",
        "            result.append(\"B\")\n",
        "  ## Write your code here\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "data = pd.read_csv('data_python.csv')\n",
        "\n",
        "V = data['Visible'].values\n",
        "\n",
        "# Transition Probabilities\n",
        "a = np.ones((2, 2))\n",
        "a = a / np.sum(a, axis=1)\n",
        "\n",
        "# Emission Probabilities\n",
        "b = np.array(((1, 3, 5), (2, 4, 6)))\n",
        "b = b / np.sum(b, axis=1).reshape((-1, 1))\n",
        "\n",
        "# Equal Probabilities for the initial distribution\n",
        "initial_distribution = np.array((0.5, 0.5))\n",
        "\n",
        "a, b = baum_welch(V, a, b, initial_distribution, n_iter=100)\n",
        "\n",
        "result = viterbi(V, a, b, initial_distribution)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wTG6SkvKOHl"
      },
      "source": [
        "4. Use the built-in **hmmlearn** package to fit the data and generate the result using the decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "R1iy8tj4KrwF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: hmmlearn in /home/abhishekj/.local/lib/python3.9/site-packages (0.2.7)\n",
            "Requirement already satisfied: scikit-learn>=0.16 in /home/abhishekj/.local/lib/python3.9/site-packages (from hmmlearn) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.10 in /home/abhishekj/.local/lib/python3.9/site-packages (from hmmlearn) (1.23.2)\n",
            "Requirement already satisfied: scipy>=0.19 in /home/abhishekj/.local/lib/python3.9/site-packages (from hmmlearn) (1.9.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/abhishekj/.local/lib/python3.9/site-packages (from scikit-learn>=0.16->hmmlearn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /home/abhishekj/.local/lib/python3.9/site-packages (from scikit-learn>=0.16->hmmlearn) (1.1.0)\n",
            "\u001b[33mWARNING: Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
            "distutils: /home/abhishekj/.local/lib/python3.9/site-packages\n",
            "sysconfig: /home/abhishekj/.local/lib64/python3.9/site-packages\u001b[0m\n",
            "\u001b[33mWARNING: Additional context:\n",
            "user = True\n",
            "home = None\n",
            "root = None\n",
            "prefix = None\u001b[0m\n",
            "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 22.3 is available.\n",
            "You should consider upgrading via the '/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install hmmlearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Pw-zo9opLLlh"
      },
      "outputs": [],
      "source": [
        "## Write your code here\n",
        "from hmmlearn import hmm\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "data = pd.read_csv('data_python.csv')\n",
        "\n",
        "V = data['Visible'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(500,)\n",
            "(1, 500)\n"
          ]
        }
      ],
      "source": [
        "print(V.shape)\n",
        "V_reshaped = np.array(V.reshape(-1,1)).T\n",
        "print(V_reshaped.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['B', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'A', 'B', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'B', 'A', 'B', 'B', 'B', 'B', 'A', 'B', 'A', 'B', 'B', 'B', 'B', 'A', 'B', 'B', 'B', 'B', 'A', 'B', 'B', 'A', 'B', 'B', 'B', 'A', 'B', 'B', 'B', 'A', 'B', 'B', 'B', 'A', 'B', 'A', 'B', 'B', 'A', 'B', 'A', 'A', 'A', 'B', 'A', 'B', 'A', 'B', 'B', 'B', 'A', 'A', 'B', 'A', 'A', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'B', 'A', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'A', 'B', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'A', 'A', 'B', 'A', 'B', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'B', 'A', 'B', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'B', 'A', 'B', 'B', 'B', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'A', 'B', 'B', 'A', 'A', 'B', 'A', 'A', 'A', 'B', 'B', 'B', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'A', 'A', 'B', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'A', 'B', 'A', 'B', 'B', 'B', 'A', 'B', 'B', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'A', 'B', 'A', 'B', 'B', 'A', 'B', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'A', 'A', 'B', 'B', 'A', 'A', 'A', 'B', 'B', 'B', 'A', 'A', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'A', 'A', 'B', 'A', 'B', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'A', 'B', 'B', 'B', 'A', 'A', 'B', 'B', 'A', 'B', 'B', 'A', 'B', 'B', 'B', 'B', 'B', 'A', 'B', 'A', 'B', 'A', 'A', 'A', 'B', 'A', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'A', 'B', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'A', 'B', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'B', 'A', 'A', 'B', 'A', 'A', 'B', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'B', 'B', 'A', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'B', 'A', 'A', 'B', 'A', 'A', 'A', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'A', 'B', 'B', 'B', 'B', 'A', 'B', 'B', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'A', 'B', 'A', 'A', 'A', 'B', 'A', 'B', 'A', 'A', 'B', 'B', 'A', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'A', 'B', 'B', 'B', 'A', 'B', 'A', 'A', 'B', 'B', 'B', 'A', 'B', 'B', 'A', 'A', 'B', 'A']\n"
          ]
        }
      ],
      "source": [
        "model = hmm.MultinomialHMM(n_components=2)\n",
        "\n",
        "model.startprob_ = np.array([0.5, 0.5])\n",
        "model.transmat_ = np.array([[0.5, 0.5],\n",
        "                            [0.5, 0.5]])\n",
        "model.emissionprob_ = np.array([[0.11111111, 0.33333333, 0.55555556],\n",
        "                                [0.16666667, 0.33333333 ,0.5]])\n",
        "\n",
        "logprob, sequence = model.decode(V_reshaped)\n",
        "out = []\n",
        "for i in sequence:\n",
        "  if i == 1:\n",
        "    i = \"B\"\n",
        "  else:\n",
        "    i = \"A\"\n",
        "  out.append(i)\n",
        "print(out)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Lab_11_Hidden_Markov_Model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
