{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZDMbDP0uJN0"
   },
   "source": [
    "# **LAB 10 : Hidden Markov Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MKFudLjzt_K5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64j_rFZeF57i"
   },
   "source": [
    "Please refer to the following [article](http://www.adeveloperdiary.com/data-science/machine-learning/introduction-to-hidden-markov-model/) to understand Hidden Markov Model\n",
    "\n",
    "Here we will be dealing with 3 major problems : \n",
    "  \n",
    "  1. Evaluation Problem\n",
    "  2. Learning Problem\n",
    "  3. Decoding Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wEasU5TNGNbE"
   },
   "source": [
    "1. Evaluation Problem : Implementation of Forward and Backward Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM\n",
    "\n",
    "- Mathematically we can say, the probability of the state at time t will only depend on time step t-1\n",
    "\n",
    "- The probability of one state changing to another state is defined as Transition Probability.\n",
    "\n",
    "-  When the system is fully observable and autonomous it’s called as Markov Chain. \n",
    "\n",
    "## Markov chain\n",
    "\n",
    "- A set of M states.\n",
    "- A transition probability matrix A.\n",
    "- An initial probability distribution π.\n",
    "\n",
    "## Transitions\n",
    "<p align=left>\n",
    "<a>\n",
    "<img src=\"imgs/Transitions.png\" width=400 height=400>\n",
    "</a>\n",
    "</p>\n",
    "\n",
    "\n",
    "## Emissions\n",
    "<p align=left>\n",
    "<a>\n",
    "<img src=\"imgs/emission.jpg\" width=400 height=400>\n",
    "</a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vjUe9joAGXvC",
    "outputId": "0a1ef95b-9653-4518-adbc-d288b6343da5"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_python.csv') ## Read the data, change the path accordingly\n",
    "\n",
    "V = data['Visible'].values\n",
    "\n",
    "# Transition Probabilities\n",
    "a = np.array(((0.54, 0.46), (0.49, 0.51)))\n",
    "\n",
    "# Emission Probabilities\n",
    "b = np.array(((0.16, 0.26, 0.58), (0.25, 0.28, 0.47)))\n",
    "\n",
    "# Equal Probabilities for the initial distribution\n",
    "initial_distribution = np.array((0.5, 0.5))\n",
    "\n",
    "\n",
    "def forward(V, a, b, initial_distribution):\n",
    "    alpha = np.zeros((V.shape[0], a.shape[0]))\n",
    "    \n",
    "    ## Write your code here\n",
    "    alpha[0, :] = initial_distribution * b[:, V[0]]\n",
    " \n",
    "    for t in range(1, V.shape[0]):\n",
    "        for j in range(a.shape[0]):\n",
    "            alpha[t, j] = alpha[t - 1].dot(a[:, j]) * b[j, V[t]]\n",
    " \n",
    "    return alpha\n",
    "\n",
    "\n",
    "alpha = forward(V, a, b, initial_distribution)\n",
    "\n",
    "\n",
    "def backward(V, a, b):\n",
    "    beta = np.zeros((V.shape[0], a.shape[0]))\n",
    "\n",
    "    ## Write your code here\n",
    "    beta[V.shape[0] - 1] = np.ones((a.shape[0]))\n",
    " \n",
    "    \n",
    "    for t in range(V.shape[0] - 2, -1, -1):\n",
    "        for j in range(a.shape[0]):\n",
    "            beta[t, j] = (beta[t + 1] * b[:, V[t + 1]]).dot(a[j, :])\n",
    " \n",
    "    return beta\n",
    "\n",
    "\n",
    "beta = backward(V, a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Training in HMM\n",
    "\n",
    "- The model is $\\theta \\rightarrow s, v, a_{ij},b_{jk}$\n",
    "\n",
    "- There could be many $\\{ \\theta_1, \\theta_2 … \\theta_n \\}$ we need to find the best based on $p(\\theta | V^T  ) = \\frac{p(V^T | \\theta) p(\\theta)}{p(V^T)}$\n",
    "\n",
    "## Training\n",
    "\n",
    "- Once the high-level structure (Number of Hidden & Visible States) of the model is defined, we need to estimate the Transition ($a_{ij}$) & Emission ($b_{jk}$) Probabilities using the training sequences.\n",
    "\n",
    "- Expectation Maximization (EM) algorithm will be used to estimate the Transition ($a_{ij}$) & Emission ($b_{jk}$) Probabilities\n",
    "\n",
    "## Forward Alg.\n",
    "\n",
    "- In Forward Algorithm, we will use the computed probability on current time step to derive the probability of the next time step.\n",
    "\n",
    "- $\\alpha_j(t) = p(v(1)…v(t),s(t)= j)$\n",
    "\n",
    "## Backward Alg.\n",
    "\n",
    "- Backward Algorithm is the time-reversed version of the Forward Algorithm. In Backward Algorithm we need to find the probability that the machine will be in hidden state ($s_{i}$)\n",
    " at time step t and will generate the remaining part of the sequence of the visible symbol ($V^{T}$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Iyc4mIlGzPk"
   },
   "source": [
    "2. Learning Problem : Implementation of Baum Welch Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM Alg. / baum_welch\n",
    "\n",
    "- initialize A and B\n",
    "\n",
    "- iterate until convergence\n",
    "    - E-Step\n",
    "        - $\\xi_{ij} (t) = \\frac{\\alpha_i(t) a_{ij} b_{jk \\text{ } v(t+1) }\\beta_j(t+1)}{\\sum_{i=1}^{M} \\sum_{j=1}^{M} \\alpha_i(t) a_{ij} b_{jk \\text{ } v(t+1) }\\beta_j(t+1)}$\n",
    "\n",
    "        - $\\gamma_i(t) = \\sum_{j=1}^M \\xi_{ij}(t)$\n",
    "\n",
    "    \n",
    "    - M-step\n",
    "        - $\\hat{a_{ij}} = \\frac{\\sum_{t=1}^{T-1} \\xi_{ij} (t)}{\\sum_{t=1}^{T-1} \\sum_{j=1}^{M} \\xi_{ij} (t)}$\n",
    "\n",
    "        - $\\hat{b_{jk}} = \\frac{\\sum_{t=1}^T \\gamma_j(t) 1(v(t)=k)}{\\sum_{t=1}^T \\gamma_j(t) }$\n",
    "\n",
    "- return A, B\n",
    "\n",
    "Here:\n",
    "\n",
    "- $\\xi$ is expectation of how often each transition is used.\n",
    "\n",
    "- $\\gamma$ is expectation of how often each hidden state is emmited.\n",
    "\n",
    "- $\\xi_{ij} (t)$ = $p(s(t) = i,s(t+1)=j | V^T, \\theta )$\n",
    "\n",
    "- $\\gamma_i(t) = \\sum_{j=1}^M \\xi_{ij}(t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H6eaCtNEG3FQ",
    "outputId": "143855ca-81f4-4bfb-97f9-5a40596c15eb"
   },
   "outputs": [],
   "source": [
    "def baum_welch(V, a, b, initial_distribution, n_iter=100):\n",
    "    M = a.shape[0]\n",
    "    T = len(V)\n",
    " \n",
    "    for n in range(n_iter):\n",
    "        alpha = forward(V, a, b, initial_distribution)\n",
    "        beta = backward(V, a, b)\n",
    " \n",
    "        xi = np.zeros((M, M, T - 1))\n",
    "        for t in range(T - 1):\n",
    "            denominator = np.dot(np.dot(alpha[t, :].T, a) * b[:, V[t + 1]].T, beta[t + 1, :])\n",
    "            for i in range(M):\n",
    "                numerator = alpha[t, i] * a[i, :] * b[:, V[t + 1]].T * beta[t + 1, :].T\n",
    "                xi[i, :, t] = numerator / denominator\n",
    " \n",
    "        gamma = np.sum(xi, axis=1)\n",
    "        a = np.sum(xi, 2) / np.sum(gamma, axis=1).reshape((-1, 1))\n",
    " \n",
    "        \n",
    "        gamma = np.hstack((gamma, np.sum(xi[:, :, T - 2], axis=0).reshape((-1, 1))))\n",
    " \n",
    "        K = b.shape[1]\n",
    "        denominator = np.sum(gamma, axis=1)\n",
    "        for l in range(K):\n",
    "            b[:, l] = np.sum(gamma[:, V == l], axis=1)\n",
    " \n",
    "        b = np.divide(b, denominator.reshape((-1, 1)))\n",
    "\n",
    "    return (a,b)\n",
    "\n",
    "data = pd.read_csv('data_python.csv')\n",
    "\n",
    "V = data['Visible'].values\n",
    "\n",
    "# Transition Probabilities\n",
    "a = np.ones((2, 2))\n",
    "a = a / np.sum(a, axis=1)\n",
    "\n",
    "# Emission Probabilities\n",
    "b = np.array(((1, 3, 5), (2, 4, 6)))\n",
    "b = b / np.sum(b, axis=1).reshape((-1, 1))\n",
    "\n",
    "# Equal Probabilities for the initial distribution\n",
    "initial_distribution = np.array((0.5, 0.5))\n",
    "\n",
    "a,b = baum_welch(V, a, b, initial_distribution, n_iter=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding the problem\n",
    "\n",
    "Finally, once we have the estimates for Transition ($a_{ij}$) & Emission ($b_{jk}$) Probabilities, we can then use the model ($\\theta$) to predict the Hidden States ($W^{T}$) which generated the Visible Sequence ($V^{T}$). The Decoding Problem is also known as Viterbi Algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A40-TPXZHTHE"
   },
   "source": [
    "3. Decoding Problem : Implementation of Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding Problem\n",
    "\n",
    "- Given a sequence of visible symbol $V^{T}$ and model $\\theta \\rightarrow \\{ A, B \\}$ find most probable hidden states.\n",
    "\n",
    "- Brute force $O(N^{T} . T)$\n",
    "\n",
    "- Viterbi Algorithm \n",
    "\n",
    "# Viterbi Algorithm\n",
    "\n",
    "-  We need to find the most probable hidden state in every iteration of t\n",
    "\n",
    "- Once we complete the above steps for all the observations, we will first find the last hidden state by maximum likelihood, then using backpointer to backtrack the most likely hidden path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nBG74dknHYgM",
    "outputId": "1ac04c53-fea0-4047-aae0-768c6c6e0780"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result is :\n",
      "['B', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\n"
     ]
    }
   ],
   "source": [
    "def viterbi(V, a, b, initial_distribution):\n",
    "    T = V.shape[0]\n",
    "    M = a.shape[0]\n",
    " \n",
    "    omega = np.zeros((T, M))\n",
    "    omega[0, :] = np.log(initial_distribution * b[:, V[0]])\n",
    " \n",
    "    prev = np.zeros((T - 1, M))\n",
    " \n",
    "    for t in range(1, T):\n",
    "        for j in range(M):\n",
    "            # Same as Forward Probability\n",
    "            probability = omega[t - 1] + np.log(a[:, j]) + np.log(b[j, V[t]])\n",
    " \n",
    "            # This is our most probable state given previous state at time t (1)\n",
    "            prev[t - 1, j] = np.argmax(probability)\n",
    " \n",
    "            # This is the probability of the most probable state (2)\n",
    "            omega[t, j] = np.max(probability)\n",
    " \n",
    "    # Path Array\n",
    "    S = np.zeros(T)\n",
    " \n",
    "    # Find the most probable last hidden state\n",
    "    last_state = np.argmax(omega[T - 1, :])\n",
    " \n",
    "    S[0] = last_state\n",
    " \n",
    "    backtrack_index = 1\n",
    "    for i in range(T - 2, -1, -1):\n",
    "        S[backtrack_index] = prev[i, int(last_state)]\n",
    "        last_state = prev[i, int(last_state)]\n",
    "        backtrack_index += 1\n",
    " \n",
    "    # Flip the path array since we were backtracking\n",
    "    S = np.flip(S, axis=0)\n",
    " \n",
    "    # Convert numeric values to actual hidden states\n",
    "    result = []\n",
    "    for s in S:\n",
    "        if s == 0:\n",
    "            result.append(\"A\")\n",
    "        else:\n",
    "            result.append(\"B\")\n",
    "  ## Write your code here\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "data = pd.read_csv('data_python.csv')\n",
    "\n",
    "V = data['Visible'].values\n",
    "\n",
    "# Transition Probabilities\n",
    "a = np.ones((2, 2))\n",
    "a = a / np.sum(a, axis=1)\n",
    "\n",
    "# Emission Probabilities\n",
    "b = np.array(((1, 3, 5), (2, 4, 6)))\n",
    "b = b / np.sum(b, axis=1).reshape((-1, 1))\n",
    "\n",
    "# Equal Probabilities for the initial distribution\n",
    "initial_distribution = np.array((0.5, 0.5))\n",
    "\n",
    "a, b = baum_welch(V, a, b, initial_distribution, n_iter=100)\n",
    "\n",
    "result = viterbi(V, a, b, initial_distribution)\n",
    "\n",
    "print(\"Result is :\")\n",
    "print(result[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wTG6SkvKOHl"
   },
   "source": [
    "4. Use the built-in **hmmlearn** package to fit the data and generate the result using the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "R1iy8tj4KrwF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: hmmlearn in /home/abhishekj/.local/lib/python3.9/site-packages (0.2.7)\n",
      "Requirement already satisfied: numpy>=1.10 in /home/abhishekj/.local/lib/python3.9/site-packages (from hmmlearn) (1.23.2)\n",
      "Requirement already satisfied: scipy>=0.19 in /home/abhishekj/.local/lib/python3.9/site-packages (from hmmlearn) (1.9.1)\n",
      "Requirement already satisfied: scikit-learn>=0.16 in /home/abhishekj/.local/lib/python3.9/site-packages (from hmmlearn) (1.1.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/abhishekj/.local/lib/python3.9/site-packages (from scikit-learn>=0.16->hmmlearn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /home/abhishekj/.local/lib/python3.9/site-packages (from scikit-learn>=0.16->hmmlearn) (1.1.0)\n",
      "\u001b[33mWARNING: Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "distutils: /home/abhishekj/.local/lib/python3.9/site-packages\n",
      "sysconfig: /home/abhishekj/.local/lib64/python3.9/site-packages\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = True\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 22.3 is available.\n",
      "You should consider upgrading via the '/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install hmmlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Pw-zo9opLLlh"
   },
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "from hmmlearn import hmm\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "data = pd.read_csv('data_python.csv')\n",
    "\n",
    "V = data['Visible'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500,)\n",
      "(1, 500)\n"
     ]
    }
   ],
   "source": [
    "print(V.shape)\n",
    "V_reshaped = np.array(V.reshape(-1,1)).T\n",
    "print(V_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'A', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'A', 'B', 'B', 'B', 'B', 'A', 'A', 'A', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'B', 'A', 'B', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'A', 'B', 'B', 'B', 'A', 'B', 'A', 'A', 'A', 'A', 'B', 'A', 'B', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'B', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'B', 'A', 'B', 'A', 'A', 'B', 'A', 'B', 'B', 'B', 'A', 'B', 'A', 'B', 'A', 'A', 'A', 'B', 'B', 'A', 'B', 'B', 'A', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'A', 'B', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'A', 'B', 'B', 'B', 'B', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'A', 'B', 'A', 'B', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'A', 'B', 'B', 'B', 'B', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'A', 'B', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'B', 'B', 'A', 'B', 'B', 'B', 'A', 'A', 'A', 'B', 'B', 'B', 'A', 'A', 'A', 'A', 'B', 'B', 'A', 'B', 'B', 'B', 'A', 'B', 'B', 'B', 'B', 'A', 'B', 'A', 'A', 'A', 'B', 'A', 'A', 'B', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'B', 'A', 'A', 'B', 'A', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'A', 'A', 'B', 'B', 'B', 'A', 'A', 'A', 'B', 'B', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'B', 'A', 'A', 'B', 'B', 'A', 'B', 'A', 'B', 'B', 'B', 'A', 'B', 'B', 'B', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'B', 'B', 'A', 'A', 'B', 'A', 'A', 'B', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'B', 'A', 'B', 'B', 'B', 'A', 'B', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'B', 'B', 'B', 'B', 'B', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'B', 'A', 'A', 'B', 'A', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'A', 'B', 'B', 'A', 'B', 'B', 'A', 'B', 'B', 'B', 'B', 'A', 'B', 'B', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'B', 'B', 'A', 'B', 'B', 'B', 'A', 'A', 'B', 'A', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'A', 'B', 'B', 'B', 'A', 'B', 'B', 'A', 'B', 'B', 'B', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'B', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'B', 'B', 'B', 'A', 'A', 'A', 'A', 'B', 'A', 'B', 'B', 'B', 'A', 'B', 'A', 'B', 'B', 'A', 'A', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'B', 'A', 'B', 'B', 'A', 'A', 'A', 'B', 'A', 'A', 'B', 'B', 'A', 'B']\n"
     ]
    }
   ],
   "source": [
    "model = hmm.MultinomialHMM(n_components=2)\n",
    "\n",
    "model.startprob_ = np.array([0.5, 0.5])\n",
    "model.transmat_ = np.array([[0.5, 0.5],\n",
    "                            [0.5, 0.5]])\n",
    "model.emissionprob_ = np.array([[0.11111111, 0.33333333, 0.55555556],\n",
    "                                [0.16666667, 0.33333333 ,0.5]])\n",
    "\n",
    "logprob, sequence = model.decode(V_reshaped)\n",
    "out = []\n",
    "for i in sequence:\n",
    "  if i == 1:\n",
    "    i = \"A\"\n",
    "  else:\n",
    "    i = \"B\"\n",
    "  out.append(i)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339\n"
     ]
    }
   ],
   "source": [
    "Real = data[\"Hidden\"].values\n",
    "\n",
    "count = 0\n",
    "for i in range(Real.shape[0]):\n",
    "    if Real[i] == out[i]:\n",
    "        count+=1\n",
    "\n",
    "print(count)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab_11_Hidden_Markov_Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
