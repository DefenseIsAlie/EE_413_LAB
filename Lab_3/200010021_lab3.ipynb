{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "va8kw6_8Qykv"
      },
      "source": [
        "# Lab 3 : Convex Optimisation\n",
        "\n",
        "### Convex function:\n",
        "1. A function $f(x)$ is called convex if the line segment between any two points on the graph of the function lies above the graph between the two points.\n",
        "\n",
        "### Gradient Descent Method: \n",
        "\n",
        "1. Gradient Descent is an iterative algorithm to find local minima of differentiable functions.\n",
        "\n",
        "2. The method repeatedly steps in the direction of steepest descent. \n",
        "\n",
        "3. The direction of steepest descent is opposite direction of gradient.\n",
        "\n",
        "###### Drawbacks:\n",
        "\n",
        "1. Gets stuck on saddle points.\n",
        "\n",
        "2. Gets stuck on local minima. \n",
        "\n",
        "### Convex Optimisation:\n",
        "\n",
        "1. Convex optimisation is the process of minimising convex functions.\n",
        "\n",
        "2. Here we use gradient descent to do convex optimisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Question 1) Single variable gradient descent\n",
        "\n",
        "### A) $f(x)$ = $x^{2} + x + 2$ \n",
        "\n",
        "1. Find x analytically\n",
        "2. Write the update equation\n",
        "3. Find x using gradient descent method\n",
        "\n",
        "### B) $f(x)$ = $xSin(x)$\n",
        "\n",
        "1. Find x analytically\n",
        "2. Write the update equation\n",
        "3. Find x using gradient descent method\n",
        "\n",
        "##### Gradient Descent Method:\n",
        "1. Generate x, 1000 points from -10 to 10\n",
        "2. Generate and plot $f(x)$\n",
        "3. Initialize starting point $(x_{init})$ and learning rate $(\\lambda)$\n",
        "4. Use gradient descent algorithm to compute value of x where, $f(x)$ is minimum\n",
        "5. Vary learning rate and initial point and plot observations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part A) \n",
        "\n",
        "##### 1) Analytical Solution\n",
        "\n",
        "1. Given $f(x) = x^{2} + x + 2$\n",
        "\n",
        "2. We find minima of $f(x)$ at critical points (where $f'(x) = 0 $ and $f''(x) > 0$)\n",
        "\n",
        "3. $f'(x)$ = $ 2x + 1 $  found after differentiating $f(x)$ similarly $f''(x) = 2 > 0$\n",
        "\n",
        "4. So, minima will be found at $f'(x)=0$ => $x= -1/2$\n",
        "\n",
        "5. So the anylytical solution is $x = - 0.5 $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91twagU9DQzd"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 2) Now we generate and plot f(x)\n",
        "\n",
        "1. First we generate 1000 X_points from -10 to 10 and corresponding Y_points\n",
        "\n",
        "2. Then we plot Y_points vs X_points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_fx(coefficients):\n",
        "  X_points = np.linspace(-10,10,10000) \n",
        "  Y_points = coefficients[0]*(X_points**2) + coefficients[1] * X_points + coefficients[2]\n",
        "\n",
        "  plt.plot(X_points, Y_points)\n",
        "  plt.xlabel('X points')\n",
        "  plt.ylabel('Corresponding Y points')\n",
        "  plt.title('f(x) = x^2 + x + 2')\n",
        "  plt.legend(['y=f(x)'])\n",
        "\n",
        "coeff = [1, 1, 2]\n",
        "\n",
        "plot_fx(coeff)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 3) Initialization\n",
        "\n",
        "1. From plot we see that $x_{min}$ is around 0 so $x_{init} = 0$\n",
        "\n",
        "2. We take learning rate = 0.01\n",
        "\n",
        "3. Now we write the algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "v2KtA-njFCRz",
        "outputId": "af021c6d-c1b9-404e-880d-f9dba345dbd0"
      },
      "outputs": [],
      "source": [
        "x_init = 0\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "def derivative_fx(coeff,x):\n",
        "  return 2*coeff[0]*x + coeff[1]\n",
        "\n",
        "def gradient_update(coeff, x, learning_rate):\n",
        "  x -= learning_rate * derivative_fx(coeff, x)\n",
        "  return x\n",
        "\n",
        "def gradient_plot(coeff, x_hist, y_hist, learning_rate):\n",
        "    ### Plotting the gradient descent\n",
        "  X_points = np.linspace(-10,10,10000) \n",
        "  Y_points = coeff[0]*(X_points**2) + coeff[1] * X_points + coeff[2]\n",
        "\n",
        "  plt.plot(X_points, Y_points)\n",
        "  plt.xlabel('X points')\n",
        "  plt.ylabel('Corresponding Y points')\n",
        "  plt.title('f(x) = x^2 + x + 2')\n",
        "\n",
        "  x_hist = np.array(x_hist)\n",
        "  y_hist = coeff[0]*(x_hist**2) + coeff[1]*x_hist+coeff[2]\n",
        "\n",
        "  plt.plot(x_hist,y_hist)\n",
        "\n",
        "  plt.legend(['y = f(x)', f'gradient_descent lr = {learning_rate}'])\n",
        "  \n",
        "\n",
        "\n",
        "def gradient_descent(num_iter, x_guess, precision, learning_rate, show_plot = True):\n",
        "  x_now = x_guess\n",
        "  x_prev = 9999999999\n",
        "\n",
        "  x_hist = []\n",
        "  y_hist = []\n",
        "\n",
        "  iterations = []\n",
        "  iterations.append(num_iter)\n",
        "  t = False\n",
        "  for i in range(num_iter):\n",
        "    if abs(x_now - x_prev) < precision:    \n",
        "      t = i\n",
        "      break\n",
        "    \n",
        "    x_hist.append(x_now)\n",
        "    x_prev = x_now\n",
        "\n",
        "    x_now = gradient_update(coeff, x_now, learning_rate)\n",
        "\n",
        "  if t:\n",
        "    iterations.append(t)\n",
        "  if show_plot:\n",
        "    gradient_plot(coeff, x_hist, y_hist, learning_rate)\n",
        "    return x_now\n",
        "\n",
        "  x_hist = np.array(x_hist)\n",
        "  y_hist = coeff[0]*(x_hist**2) + coeff[1]*x_hist+coeff[2]\n",
        "\n",
        "  return [x_hist, y_hist, iterations[-1]]\n",
        "\n",
        "\n",
        "x_min = gradient_descent(1000, x_init, 1e-10, lr)\n",
        "\n",
        "print(f\"X_min is {x_min}\")  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 4) Now we vary learning rate and initial points and plot the graphs\n",
        "\n",
        "1. lr = [0.00001, 0.001, 0.01, 0.1, 0.3, 0.6, 1, 10]\n",
        "\n",
        "2. x_init = [12, 10, 3, 5, -1, -10, -5, -8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lr = [0.00001, 0.001, 0.01, 0.1, 0.3, 0.6, 1]\n",
        "\n",
        "x_init = [12, 10, -10, -5]\n",
        "\n",
        "\n",
        "def Vary_Learning_rate(xinit=10):\n",
        "    History = []\n",
        "    legend = ['y = f(x)']\n",
        "\n",
        "    for rate in lr:\n",
        "        History.append(gradient_descent(1000, xinit, 1e-10, rate, False))\n",
        "\n",
        "    X_points = np.linspace(-10,10,10000) \n",
        "    Y_points = coeff[0]*(X_points**2) + coeff[1] * X_points + coeff[2]\n",
        "    plt.figure(0)\n",
        "    plt.plot(X_points, Y_points)\n",
        "    plt.xlabel('X points')\n",
        "    plt.ylabel('Corresponding Y points')\n",
        "    plt.title(f'Vary lr, x_init = {xinit}')\n",
        "\n",
        "    for i in range(len(lr)):\n",
        "        x_hist  = History[i][0]\n",
        "        y_hist = History[i][1]\n",
        "        plt.plot(x_hist,y_hist)\n",
        "\n",
        "        legend.append(f\"lr = {lr[i]}, iter={History[i][2]}\")\n",
        "\n",
        "    plt.legend(legend)\n",
        "\n",
        "Vary_Learning_rate(xinit=-10)\n",
        "\n",
        "\n",
        "\n",
        "def Vary_xinit(lr=0.01):\n",
        "    History = []\n",
        "    legend = ['y = f(x)']\n",
        "\n",
        "    for init in x_init:\n",
        "        History.append(gradient_descent(1000, init, 1e-10, lr, False))\n",
        "\n",
        "    X_points = np.linspace(-10,10,10000) \n",
        "    Y_points = coeff[0]*(X_points**2) + coeff[1] * X_points + coeff[2]\n",
        "    plt.figure(1)\n",
        "    plt.plot(X_points, Y_points)\n",
        "    plt.xlabel('X points')\n",
        "    plt.ylabel('Corresponding Y points')\n",
        "    plt.title(f'Vary x_init, lr = {lr}')\n",
        "\n",
        "    for i in range(len(x_init)):\n",
        "        x_hist  = History[i][0]\n",
        "        y_hist = History[i][1]\n",
        "        plt.plot(x_hist,y_hist)\n",
        "\n",
        "        legend.append(f\"xinit = {x_init[i]}\")\n",
        "\n",
        "    plt.legend(legend)\n",
        "\n",
        "Vary_xinit()\n",
        "        \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part B) \n",
        "\n",
        "##### 1) Analytical Solution\n",
        "\n",
        "1. Given $f(x) = x Sin(x)$\n",
        "\n",
        "2. We find minima of $f(x)$ at critical points (where $f'(x) = 0 $ and $f''(x) > 0$)\n",
        "\n",
        "3. $f'(x)$ = $ Sin(x) + x Cos(x) $  found after differentiating $f(x)$ similarly $f''(x) =Sin(x) + Cos(x) - xSin(x) $\n",
        "\n",
        "4. So, minima will be found at $f'(x)=0$ => $x= - Tan(x)$\n",
        "\n",
        "5. So the anylytical solution is $x = 0$ etc...\n",
        "\n",
        "6. There are infinite local minima."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 2) Now we generate and plot f(x)\n",
        "\n",
        "1. First we generate 1000 X_points from -10 to 10 and corresponding Y_points\n",
        "\n",
        "2. Then we plot Y_points vs X_points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_fx():\n",
        "  X_points = np.linspace(-10,10,10000) \n",
        "  Y_points = X_points * np.sin(X_points)\n",
        "\n",
        "  plt.plot(X_points, Y_points)\n",
        "  plt.xlabel('X points')\n",
        "  plt.ylabel('Corresponding Y points')\n",
        "  plt.title('f(x) = xSin(x)')\n",
        "  plt.legend(['y=f(x)'])\n",
        "\n",
        "\n",
        "plot_fx()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 3) Initialization\n",
        "\n",
        "1. From plot we see that $x_{min}$ is around 0 , +/- 2.5, 7.5 so $x_{init} = 1,4,7 $\n",
        "\n",
        "2. We take learning rate = 0.01\n",
        "\n",
        "3. Now we write the algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from math import cos, sin\n",
        "\n",
        "\n",
        "x_init = 4\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "def derivative_fx(x):\n",
        "  return x*cos(x) + sin(x)\n",
        "\n",
        "def gradient_update(x, learning_rate):\n",
        "  x -= learning_rate * derivative_fx(x)\n",
        "  return x\n",
        "\n",
        "def gradient_plot(x_hist, y_hist, learning_rate):\n",
        "    ### Plotting the gradient descent\n",
        "  X_points = np.linspace(-10,10,10000) \n",
        "  Y_points = X_points * np.sin(X_points)\n",
        "\n",
        "  plt.plot(X_points, Y_points)\n",
        "  plt.xlabel('X points')\n",
        "  plt.ylabel('Corresponding Y points')\n",
        "  plt.title('f(x) = xSin(x)')\n",
        "\n",
        "  x_hist = np.array(x_hist)\n",
        "  y_hist = x_hist * np.sin(x_hist)\n",
        "\n",
        "  plt.plot(x_hist,y_hist)\n",
        "\n",
        "  plt.legend(['y = f(x)', f'gradient_descent lr = {learning_rate}'])\n",
        "  \n",
        "\n",
        "\n",
        "def gradient_descent(num_iter, x_guess, precision, learning_rate, show_plot = True):\n",
        "  x_now = x_guess\n",
        "  x_prev = 9999999999\n",
        "\n",
        "  x_hist = []\n",
        "  y_hist = []\n",
        "\n",
        "  iterations = []\n",
        "  iterations.append(num_iter)\n",
        "  t = False\n",
        "  for i in range(num_iter):\n",
        "    if abs(x_now - x_prev) < precision:    \n",
        "      t = i\n",
        "      break\n",
        "    \n",
        "    x_hist.append(x_now)\n",
        "    x_prev = x_now\n",
        "\n",
        "    x_now = gradient_update(x_now, learning_rate)\n",
        "\n",
        "  if t:\n",
        "    iterations.append(t)\n",
        "  if show_plot:\n",
        "    gradient_plot(x_hist, y_hist, learning_rate)\n",
        "    return x_now\n",
        "\n",
        "  x_hist = np.array(x_hist)\n",
        "  y_hist = x_hist * np.sin(x_hist)\n",
        "\n",
        "  return [x_hist, y_hist, iterations[-1]]\n",
        "\n",
        "\n",
        "x_min = gradient_descent(1000, x_init, 1e-10, lr)\n",
        "\n",
        "print(f\"X_min is {x_min}\")  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 4) Now we vary learning rate and initial points and plot the graphs\n",
        "\n",
        "1. lr = [0.00001, 0.001, 0.01, 0.1, 0.3, 0.6, 1, 10]\n",
        "\n",
        "2. x_init = [12, 10, 3, 5, -1, -10, -5, -8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lr = [0.00001, 0.001, 0.01, 0.1, 0.3, 0.6]\n",
        "\n",
        "x_init = [1, -1, 4, -4, 7, -7]\n",
        "\n",
        "\n",
        "def Vary_Learning_rate(xinit=4):\n",
        "    History = []\n",
        "    legend = ['y = f(x)']\n",
        "\n",
        "    for rate in lr:\n",
        "        History.append(gradient_descent(1000, xinit, 1e-10, rate, False))\n",
        "\n",
        "    X_points = np.linspace(-10,10,10000) \n",
        "    Y_points = X_points * np.sin(X_points)\n",
        "    plt.figure(0)\n",
        "    plt.plot(X_points, Y_points)\n",
        "    plt.xlabel('X points')\n",
        "    plt.ylabel('Corresponding Y points')\n",
        "    plt.title(f'Vary lr, x_init = {xinit}')\n",
        "\n",
        "    for i in range(len(lr)):\n",
        "        x_hist  = History[i][0]\n",
        "        y_hist = History[i][1]\n",
        "        plt.plot(x_hist,y_hist)\n",
        "\n",
        "        legend.append(f\"lr = {lr[i]}, iter={History[i][2]}\")\n",
        "\n",
        "    plt.legend(legend)\n",
        "\n",
        "Vary_Learning_rate(xinit=4)\n",
        "\n",
        "\n",
        "\n",
        "def Vary_xinit(lr=0.01):\n",
        "    History = []\n",
        "    legend = ['y = f(x)']\n",
        "\n",
        "    for init in x_init:\n",
        "        History.append(gradient_descent(1000, init, 1e-10, lr, False))\n",
        "\n",
        "    X_points = np.linspace(-10,10,10000) \n",
        "    Y_points = X_points * np.sin(X_points)\n",
        "    plt.figure(1)\n",
        "    plt.plot(X_points, Y_points)\n",
        "    plt.xlabel('X points')\n",
        "    plt.ylabel('Corresponding Y points')\n",
        "    plt.title(f'Vary x_init, lr = {lr}')\n",
        "\n",
        "    for i in range(len(x_init)):\n",
        "        x_hist  = History[i][0]\n",
        "        y_hist = History[i][1]\n",
        "        plt.plot(x_hist,y_hist)\n",
        "\n",
        "        legend.append(f\"xinit = {x_init[i]}\")\n",
        "\n",
        "    plt.legend(legend)\n",
        "\n",
        "Vary_xinit()\n",
        "        \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Question 2) Two variable gradient descent\n",
        "\n",
        "### A) $f(x,y)$ = $x^{2} + y^{2} + 2x + 2y$ \n",
        "\n",
        "\n",
        "1. Write the update equation\n",
        "2. Find x using gradient descent method\n",
        "\n",
        "### B) A) $f(x)$ = $x Sin(x) + y Sin(y)$\n",
        "\n",
        "\n",
        "1. Write the update equation\n",
        "2. Find x using gradient descent method\n",
        "\n",
        "##### Gradient Descent Method:\n",
        "1. Generate x and y, 1000 points from -10 to 10\n",
        "2. Generate and plot $f(x,y)$\n",
        "3. Initialize starting point $(x_{init}, y_{init})$ and learning rate $(\\lambda)$\n",
        "4. Use gradient descent algorithm to compute value of x where, $f(x,y)$ is minimum\n",
        "5. Vary learning rate and initial point and plot observations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part A)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 1) Now we generate and plot f(x,y)\n",
        "\n",
        "1. First we generate 1000 X_points, 1000 Y_points from -10 to 10 and corresponding Y_points\n",
        "\n",
        "2. Then we plot Z_points vs X_points, Y_points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mpl_toolkits import mplot3d\n",
        "\n",
        "def plot_fxy():\n",
        "  X_points = np.linspace(-10,10,200) \n",
        "  Y_points = np.linspace(-10,10,200)\n",
        "\n",
        "  X_points, Y_points = np.meshgrid(X_points, Y_points)\n",
        "\n",
        "  Z_points = X_points**2 + Y_points**2 + 2*X_points + 2*Y_points\n",
        "  fig = plt.figure()\n",
        "  ax = plt.axes(projection='3d')\n",
        "  ax.plot_trisurf(X_points.flatten(), Y_points.flatten(), Z_points.flatten())\n",
        "\n",
        "\n",
        "plot_fxy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 2) Initialization\n",
        "\n",
        "1. From plot we see that $x_{min}$ is around 0,0  so $x_{init}, y_{init} = -5,5 $\n",
        "\n",
        "2. We take learning rate = 0.01\n",
        "\n",
        "3. Now we write the algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from math import cos, sin\n",
        "\n",
        "\n",
        "x_init = -5\n",
        "y_init = 5\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "def derivative_fx(x):\n",
        "    return 2*x + 2\n",
        "\n",
        "def derivative_fy(y):\n",
        "    return 2*y + 2\n",
        "\n",
        "def gradient_update(x, y, learning_rate):\n",
        "  x -= learning_rate * derivative_fx(x)\n",
        "  y -= learning_rate * derivative_fy(y)\n",
        "  return [x,y]\n",
        "\n",
        "def gradient_plot(x_hist, y_hist, learning_rate):\n",
        "    ### Plotting the gradient descent\n",
        "  X_points = np.linspace(-10,10,100) \n",
        "  Y_points = np.linspace(-10,10,100)\n",
        "\n",
        "  X_points, Y_points = np.meshgrid(X_points, Y_points)\n",
        "\n",
        "  Z_points = X_points**2 + Y_points**2 + 2*X_points + 2*Y_points\n",
        "  ax = plt.axes(projection='3d')\n",
        "  ax.plot_trisurf(X_points.flatten(), Y_points.flatten(), Z_points.flatten())\n",
        "\n",
        "  x_hist = np.array(x_hist)\n",
        "  y_hist = np.array(y_hist)\n",
        "  z_hist = x_hist**2 + y_hist**2 + 2*x_hist + 2*y_hist\n",
        "\n",
        "  ax.plot3D(x_hist,y_hist,z_hist)\n",
        "  \n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "def gradient_descent(num_iter, x_guess, y_guess, precision, learning_rate, show_plot = True):\n",
        "  x_now = x_guess\n",
        "  y_now = y_guess\n",
        "  x_prev = 9999999999\n",
        "  y_prev = 9999999999\n",
        "\n",
        "  x_hist = []\n",
        "  y_hist = []\n",
        "  z_hist = []\n",
        "\n",
        "  iterations = []\n",
        "  iterations.append(num_iter)\n",
        "  t = False\n",
        "  for i in range(num_iter):\n",
        "    if abs(x_now - x_prev) < precision and abs(y_now - y_prev) < precision:    \n",
        "      t = i\n",
        "      break\n",
        "    \n",
        "    x_hist.append(x_now)\n",
        "    y_hist.append(y_now)\n",
        "    x_prev = x_now\n",
        "    y_prev = y_now\n",
        "    tmp = gradient_update(x_now,y_now, learning_rate)\n",
        "    x_now = tmp[0]\n",
        "    y_now = tmp[1]\n",
        "\n",
        "  if t:\n",
        "    iterations.append(t)\n",
        "  if show_plot:\n",
        "    gradient_plot(x_hist, y_hist, learning_rate)\n",
        "    return [x_now, y_now]\n",
        "\n",
        "  x_hist = np.array(x_hist)\n",
        "  y_hist = np.array(y_hist)\n",
        "  z_hist = x_hist**2 + y_hist**2 + 2*x_hist + 2*y_hist\n",
        "  return [x_hist, y_hist, z_hist]\n",
        "\n",
        "\n",
        "x_min = gradient_descent(1000, x_init, y_init, 1e-10, lr)\n",
        "\n",
        "print(f\"X_min is {x_min}\")  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 3) Now we vary learning rate and initial points and plot the graphs\n",
        "\n",
        "1. lr = [ 0.01, 0.1, 0.3]\n",
        "\n",
        "2. x_init = [ 10,  5,  -10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lr = [0.01, 0.1, 0.3]\n",
        "X_init = [ 10,  5,  -10]\n",
        "\n",
        "def Varylr():\n",
        "    History = []\n",
        "\n",
        "    for rate in lr:\n",
        "        History.append(gradient_descent(1000, x_init, y_init, 1e-10, rate, False))\n",
        "\n",
        "    X_points = np.linspace(-10,10,100) \n",
        "    Y_points = np.linspace(-10,10,100)\n",
        "    \n",
        "    X_points, Y_points = np.meshgrid(X_points, Y_points)\n",
        "\n",
        "    Z_points = X_points**2 + Y_points**2 + 2*X_points + 2*Y_points\n",
        "    ax = plt.axes(projection='3d')\n",
        "    ax.plot_trisurf(X_points.flatten(), Y_points.flatten(), Z_points.flatten())\n",
        "\n",
        "    for i in range(len(lr)):\n",
        "        x_hist  = History[i][0]\n",
        "        y_hist = History[i][1]\n",
        "        z_hist = History[i][2]\n",
        "        ax.plot3D(x_hist,y_hist,z_hist)\n",
        "\n",
        "Varylr()\n",
        "\n",
        "def Vary_xinit():\n",
        "    History = []\n",
        "\n",
        "    for init in X_init:\n",
        "        History.append(gradient_descent(1000, init, init, 1e-10, 0.01, False))\n",
        "\n",
        "    X_points = np.linspace(-10,10,100) \n",
        "    Y_points = np.linspace(-10,10,100)\n",
        "    X_points, Y_points = np.meshgrid(X_points, Y_points)\n",
        "\n",
        "    Z_points = X_points**2 + Y_points**2 + 2*X_points + 2*Y_points\n",
        "    plt.figure()\n",
        "    ax = plt.axes(projection='3d')\n",
        "    ax.plot_trisurf(X_points.flatten(), Y_points.flatten(), Z_points.flatten())\n",
        "\n",
        "    for i in range(len(lr)):\n",
        "        x_hist  = History[i][0]\n",
        "        y_hist = History[i][1]\n",
        "        z_hist = History[i][2]\n",
        "        ax.plot3D(x_hist,y_hist,z_hist)\n",
        "\n",
        "Vary_xinit()\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part B)\n",
        "\n",
        "##### 1) Now we generate and plot f(x,y)\n",
        "\n",
        "1. First we generate 1000 X_points, 1000 Y_points from -10 to 10 and corresponding Y_points\n",
        "\n",
        "2. Then we plot Z_points vs X_points, Y_points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mpl_toolkits import mplot3d\n",
        "\n",
        "def plot_fxy():\n",
        "  X_points = np.linspace(-10,10,200) \n",
        "  Y_points = np.linspace(-10,10,200)\n",
        "\n",
        "  X_points, Y_points = np.meshgrid(X_points, Y_points)\n",
        "\n",
        "  Z_points = X_points*np.sin(X_points) + Y_points*np.sin(Y_points)\n",
        "  fig = plt.figure()\n",
        "  ax = plt.axes(projection='3d')\n",
        "  ax.plot_trisurf(X_points.flatten(), Y_points.flatten(), Z_points.flatten())\n",
        "\n",
        "\n",
        "plot_fxy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 2) Initialization\n",
        "\n",
        "1. From plot we see that $x_{min}$ is around 0,0  so $x_{init}, y_{init} = -5,5 $\n",
        "\n",
        "2. We take learning rate = 0.01\n",
        "\n",
        "3. Now we write the algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from math import cos, sin\n",
        "\n",
        "\n",
        "x_init = -5\n",
        "y_init = 5\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "def derivative_fx(x):\n",
        "    return x*cos(x) + sin(x)\n",
        "\n",
        "def derivative_fy(y):\n",
        "    return y*cos(y) + sin(y)\n",
        "\n",
        "def gradient_update(x, y, learning_rate):\n",
        "  x -= learning_rate * derivative_fx(x)\n",
        "  y -= learning_rate * derivative_fy(y)\n",
        "  return [x,y]\n",
        "\n",
        "def gradient_plot(x_hist, y_hist, learning_rate):\n",
        "    ### Plotting the gradient descent\n",
        "  X_points = np.linspace(-10,10,100) \n",
        "  Y_points = np.linspace(-10,10,100)\n",
        "\n",
        "  X_points, Y_points = np.meshgrid(X_points, Y_points)\n",
        "\n",
        "  Z_points = X_points*np.sin(X_points) + Y_points*np.sin(Y_points)\n",
        "  ax = plt.axes(projection='3d')\n",
        "  ax.plot_trisurf(X_points.flatten(), Y_points.flatten(), Z_points.flatten())\n",
        "\n",
        "  x_hist = np.array(x_hist)\n",
        "  y_hist = np.array(y_hist)\n",
        "  z_hist = x_hist*np.sin(x_hist) + y_hist*np.sin(y_hist)\n",
        "\n",
        "  ax.plot3D(x_hist,y_hist,z_hist)\n",
        "  \n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "def gradient_descent(num_iter, x_guess, y_guess, precision, learning_rate, show_plot = True):\n",
        "  x_now = x_guess\n",
        "  y_now = y_guess\n",
        "  x_prev = 9999999999\n",
        "  y_prev = 9999999999\n",
        "\n",
        "  x_hist = []\n",
        "  y_hist = []\n",
        "  z_hist = []\n",
        "\n",
        "  iterations = []\n",
        "  iterations.append(num_iter)\n",
        "  t = False\n",
        "  for i in range(num_iter):\n",
        "    if abs(x_now - x_prev) < precision and abs(y_now - y_prev) < precision:    \n",
        "      t = i\n",
        "      break\n",
        "    \n",
        "    x_hist.append(x_now)\n",
        "    y_hist.append(y_now)\n",
        "    x_prev = x_now\n",
        "    y_prev = y_now\n",
        "    tmp = gradient_update(x_now,y_now, learning_rate)\n",
        "    x_now = tmp[0]\n",
        "    y_now = tmp[1]\n",
        "\n",
        "  if t:\n",
        "    iterations.append(t)\n",
        "  if show_plot:\n",
        "    gradient_plot(x_hist, y_hist, learning_rate)\n",
        "    return [x_now, y_now]\n",
        "\n",
        "  x_hist = np.array(x_hist)\n",
        "  y_hist = np.array(y_hist)\n",
        "  z_hist = x_hist*np.sin(x_hist) + y_hist*np.sin(y_hist)\n",
        "  return [x_hist, y_hist, z_hist]\n",
        "\n",
        "\n",
        "x_min = gradient_descent(1000, x_init, y_init, 1e-10, lr)\n",
        "\n",
        "print(f\"X_min is {x_min}\")  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 3) Now we vary learning rate and initial points and plot the graphs\n",
        "\n",
        "1. lr = [ 0.01, 0.1, 0.3]\n",
        "\n",
        "2. x_init = [ 10,  5,  -10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lr = [0.01, 0.1, 0.3]\n",
        "X_init = [ 10,  5,  -10]\n",
        "\n",
        "def Varylr():\n",
        "    History = []\n",
        "\n",
        "    for rate in lr:\n",
        "        History.append(gradient_descent(1000, x_init, y_init, 1e-10, rate, False))\n",
        "\n",
        "    X_points = np.linspace(-10,10,100) \n",
        "    Y_points = np.linspace(-10,10,100)\n",
        "    \n",
        "    X_points, Y_points = np.meshgrid(X_points, Y_points)\n",
        "\n",
        "    Z_points = X_points*np.sin(X_points) + Y_points*np.sin(Y_points)\n",
        "    ax = plt.axes(projection='3d')\n",
        "    ax.plot_trisurf(X_points.flatten(), Y_points.flatten(), Z_points.flatten())\n",
        "\n",
        "    for i in range(len(lr)):\n",
        "        x_hist  = History[i][0]\n",
        "        y_hist = History[i][1]\n",
        "        z_hist = History[i][2]\n",
        "        ax.plot3D(x_hist,y_hist,z_hist)\n",
        "\n",
        "Varylr()\n",
        "\n",
        "def Vary_xinit():\n",
        "    History = []\n",
        "\n",
        "    for init in X_init:\n",
        "        History.append(gradient_descent(1000, init, init, 1e-10, 0.01, False))\n",
        "\n",
        "    X_points = np.linspace(-10,10,100) \n",
        "    Y_points = np.linspace(-10,10,100)\n",
        "    X_points, Y_points = np.meshgrid(X_points, Y_points)\n",
        "\n",
        "    Z_points = X_points*np.sin(X_points) + Y_points*np.sin(Y_points)\n",
        "    plt.figure()\n",
        "    ax = plt.axes(projection='3d')\n",
        "    ax.plot_trisurf(X_points.flatten(), Y_points.flatten(), Z_points.flatten())\n",
        "\n",
        "    for i in range(len(lr)):\n",
        "        x_hist  = History[i][0]\n",
        "        y_hist = History[i][1]\n",
        "        z_hist = History[i][2]\n",
        "        ax.plot3D(x_hist,y_hist,z_hist)\n",
        "\n",
        "Vary_xinit()\n",
        "        "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "200010021_lab3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
